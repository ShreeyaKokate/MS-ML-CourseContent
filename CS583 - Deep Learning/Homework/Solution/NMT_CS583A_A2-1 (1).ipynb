{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_CS583A_A2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq3mhcsM7H_5"
      },
      "source": [
        "Shreeya Kokate\n",
        "\n",
        "CWID: 20005256\n",
        "\n",
        "Section: CS 583A Tuesday 9.30am-12pm\n",
        "\n",
        "\n",
        "\n",
        "Question:  Neural  Machine  Translation  (10  Points)\n",
        "You should train a neural network model to automatically translate from English words to their \n",
        "transformated forms. The transformation rules are as follows:\n",
        "\n",
        "• if the first letter is a consonant, then that letter is moved to the end of the word and “ay” is appended, e.g., slow →lowsay\n",
        "\n",
        "• if a word starts with a vowel, then append “way” at the end, e.g., amoeba →amoebaway.\n",
        "\n",
        "• some consonant pairs like “sh” are moved together to end of the word with “ay” appended, e.g., shallow →allowshay.\n",
        "\n",
        "To translate a sentence, simply translate each word independently.\n",
        "\n",
        "\n",
        "ANSWER: This is a Pig-Latin Pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLaT4P4kRtNF"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "K.clear_session()\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, SimpleRNN, Dense, LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "breZVSCysO2r"
      },
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data.txt', sep = ' ', header = None)\n",
        "dataset.columns = ['SourceWords', 'TargetWords']\n",
        "dataset['SourceWords'] = dataset['SourceWords'].astype(str).apply(lambda x:\" \".join(list(x)))\n",
        "dataset['TargetWords'] = dataset['TargetWords'].astype(str).apply(lambda x:\" \".join(list(x)))\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUHlTp2YRtGy"
      },
      "source": [
        "dataset.columns = ('EncoderSourceWords', 'TargetWords')\n",
        "dataset['DecoderSourceWords'] = dataset['TargetWords'].apply(lambda x: '<begin> '+ x[:-1])\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuh0ASMfRs71"
      },
      "source": [
        "train_dataset, test_dataset = train_test_split(dataset, test_size = 0.2)\n",
        "\n",
        "X_train_encoder = [[vocab[char] for char in x.split() if vocab[char] != 0] for x in train_dataset[\"EncoderSourceWords\"].values]\n",
        "X_test_encoder = [[vocab[char] for char in x.split() if vocab[char] != 0] for x in test_dataset[\"EncoderSourceWords\"].values]\n",
        "\n",
        "X_train_decoder = [[vocab[char] for char in x.split() if vocab[char] != 0] for x in train_dataset[\"DecoderSourceWords\"].values]\n",
        "X_test_decoder = [[vocab[char] for char in x.split() if vocab[char] != 0] for x in test_dataset[\"DecoderSourceWords\"].values]\n",
        "\n",
        "Y_train = [[vocab[char] for char in x.split() if vocab[char] !=0] for x in train_dataset[\"TargetWords\"].values]\n",
        "Y_test = [[vocab[char] for char in x.split() if vocab[char] !=0] for x in test_dataset[\"TargetWords\"].values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BgkfSJ_Rs2a"
      },
      "source": [
        "X_train_encoder = sequence.pad_sequences(X_train_encoder, maxlen = 20)\n",
        "X_test_encoder = sequence.pad_sequences(X_test_encoder, maxlen = 20)\n",
        "\n",
        "X_train_decoder = sequence.pad_sequences(X_train_decoder, maxlen = 20)\n",
        "X_test_decoder = sequence.pad_sequences(X_test_decoder, maxlen = 20)\n",
        "\n",
        "Y_train = sequence.pad_sequences(Y_train, maxlen = 20)\n",
        "Y_test = sequence.pad_sequences(Y_test, maxlen = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEQqu4pPRtBW"
      },
      "source": [
        "count = 1\n",
        "vocab_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', '`', '<begin>']\n",
        "vocab = defaultdict(lambda : 0)\n",
        "for i in vocab_list:\n",
        "    if (vocab[i] == 0): \n",
        "        vocab[i] = count\n",
        "        count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INddBr9hRswr"
      },
      "source": [
        "def Encoder(vocab_size = 30 + 1, embedding_size = 10, input_len= 20):\n",
        "    EncoderSource = Input(shape = (20,))\n",
        "    a = Embedding(vocab_size, embedding_size)(EncoderSource)\n",
        "    encoder = SimpleRNN(embedding_size, return_state = True)\n",
        "        \n",
        "    EncoderTarget, final_state = encoder(a)\n",
        "    return EncoderSource, EncoderTarget, final_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z99Egv1YRsuC"
      },
      "source": [
        "def Decoder(final_state, vocab_size = 30 + 1, embedding_size = 10, input_len = 20):\n",
        "    DecoderSource = Input(shape = (20,))\n",
        "    a = Embedding(vocab_size, embedding_size)(DecoderSource)\n",
        "    decoder = SimpleRNN(embedding_size, return_state = True, return_sequences = True)\n",
        "\n",
        "    model_output, _ = decoder(a, initial_state = final_state)\n",
        "    DecoderTarget = Dense(vocab_size, activation = 'softmax')(model_output)\n",
        "    return DecoderSource, DecoderTarget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz21AAlIRsrR"
      },
      "source": [
        "EncoderSource, EncoderTarget, final_state  = Encoder()\n",
        "DecoderSource, DecoderTarget = Decoder(final_state)\n",
        "model = Model([EncoderSource, DecoderSource], DecoderTarget)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a6CDBBdRsoj"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9AbNulSRsl4"
      },
      "source": [
        "model.fit([X_train_encoder, X_train_decoder], Y_train, batch_size = 16, epochs = 30, verbose = 2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ymOmnNRsgo"
      },
      "source": [
        "Y_Predict = model.predict([X_test_encoder, X_test_decoder])\n",
        "\n",
        "metric = keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
        "metric.update_state(Y_test,Y_Predict)\n",
        "\n",
        "print('Accuracy = {}'.format(metric.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmjDEgIY288-"
      },
      "source": [
        "Q4. Question:  If you input a very long sentence,  do you have problem of translation this sentence? If so, what could be potentially the problem? Write down the answer. \n",
        "\n",
        "ANSWER: \n",
        "\n",
        "If we input a very long sentence, we will surely have problem of translation in this sentence.\n",
        "\n",
        "Attempting to back-propagate across very long input sequences may result in vanishing gradients, and in turn, an unlearnable model. This may result in the problem of very long training times.\n",
        "\n",
        "This may be happening due to activation function of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V71ROcZ4R36"
      },
      "source": [
        "Q5. What model or activation function can you use the improve long sentence translation?  Implement such a model. \n",
        "\n",
        "ANSWER: \n",
        "\n",
        "Long Short-Term Memory or LSTM recurrent neural networks are capable of learning and remembering over long sequences of inputs.\n",
        "\n",
        "Afew techniques can be:\n",
        "1. Truncate Sequences\n",
        "2. Use Truncated Backpropagation Through Time\n",
        "3. Sometimes, summarize the input sequences\n",
        "4. Random Sampling\n",
        "5. Use an Encoder-Decoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LJxIDgyRsd7"
      },
      "source": [
        "def Encoder(vocab_size = 30 + 1, embedding_size = 10, input_len= 20):\n",
        "    EncoderSource = Input(shape = (20,))\n",
        "    b = Embedding(vocab_size, embedding_size)(EncoderSource)\n",
        "    encoder = LSTM(embedding_size, return_state=True)\n",
        "        \n",
        "    EncoderTarget, final_state_h, final_state_c = encoder(b)\n",
        "    return EncoderSource, EncoderTarget, [final_state_h, final_state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4hMj5IjZNci"
      },
      "source": [
        "def Decoder(final_state, vocab_size = 30 + 1, embedding_size = 10, input_len = 20):\n",
        "    DecoderSource = Input(shape = (20,))\n",
        "    a = Embedding(vocab_size, embedding_size)(DecoderSource)\n",
        "    decoder = LSTM(embedding_size, return_state = True, return_sequences = True)\n",
        "\n",
        "    model_output, _,_ = decoder(a, initial_state = final_state)\n",
        "    DecoderTarget = Dense(vocab_size, activation = 'softmax')(model_output)\n",
        "    return DecoderSource, DecoderTarget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDHffvaOZNVr"
      },
      "source": [
        "EncoderSource, EncoderTarget, final_state  = Encoder()\n",
        "DecoderSource, DecoderTarget = Decoder(final_state)\n",
        "model = Model([EncoderSource, DecoderSource], DecoderTarget)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR656J0PZNSd"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')\n",
        "model.fit([X_train_encoder, X_train_decoder], Y_train, batch_size = 16, epochs = 30, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjf77T2FZNPd"
      },
      "source": [
        "Y_Predict = model.predict([X_test_encoder, X_test_decoder])\n",
        "\n",
        "metric = keras.metrics.SparseTopKCategoricalAccuracy(k = 1)\n",
        "metric.update_state(Y_test,Y_Predict)\n",
        "\n",
        "print('Accuracy = {}'.format(metric.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}