{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRY_8YZotR7s"
      },
      "source": [
        "# HW 6: Clustering and Topic Modeling\n",
        "\n",
        "Shreeya Kokate \n",
        "\n",
        "CWID: 20005256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHygbBvTtR7x"
      },
      "source": [
        "In this assignment, you'll practice different text clustering methods. A dataset has been prepared for you:\n",
        "- `hw6_train.csv`: This file contains a list of documents. It's used for training models\n",
        "- `hw6_test`: This file contains a list of documents and their ground-truth labels (4 lables: 1,2,3,7). It's used for external evaluation. \n",
        "\n",
        "|Text| Label|\n",
        "|----|-------|\n",
        "|paraglider collides with hot air balloon ... | 1|\n",
        "|faa issues fire warning for lithium ... | 2|\n",
        "| .... |...|\n",
        "\n",
        "Sample outputs have been provided to you. Due to randomness, you may not get the same result as shown here. Your taget is to achieve about 70% F1 for the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqYBe0LptR7y"
      },
      "source": [
        "## Q1: K-Mean Clustering \n",
        "\n",
        "Define a function `cluster_kmean(train_text, test_text, text_label)` as follows:\n",
        "- Take three inputs: \n",
        "    - `train_text` is a list of documents for traing \n",
        "    - `test_text` is a list of documents for test\n",
        "    - `test_label` is the labels corresponding to documents in `test_text` \n",
        "- First generate `TFIDF` weights. You need to decide appropriate values for parameters such as `stopwords` and `min_df`:\n",
        "    - Keep or remove stopwords? Customized stop words? \n",
        "    - Set appropriate `min_df` to filter infrequent words\n",
        "- Use `KMeans` to cluster documents in `train_text` into 4 clusters. Here you need to decide the following parameters:\n",
        "    \n",
        "    - Distance measure: `cosine similarity`  or `Euclidean distance`? Pick the one which gives you better performance.  \n",
        "    - When clustering, be sure to  use sufficient iterations with different initial centroids to make sure clustering converge.\n",
        "- Test the clustering model performance using `test_label` as follows: \n",
        "  - Predict the cluster ID for each document in `test_text`.\n",
        "  - Apply `majority vote` rule to dynamically map the predicted cluster IDs to `test_label`. Note, you'd better not hardcode the mapping, because cluster IDs may be assigned differently in each run. (hint: if you use pandas, look for `idxmax` function).\n",
        "  - print out the classification report for the test subset \n",
        "  \n",
        "  \n",
        "- This function has no return. Print out the classification report. \n",
        "\n",
        "\n",
        "- Briefly discuss:\n",
        "    - Which distance measure is better and why it is better. \n",
        "    - Could you assign a meaningful name to each cluster? Discuss how you interpret each cluster.\n",
        "- Write your analysis in a pdf file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7bIvzGTF71i",
        "outputId": "97d4fa15-3312-429e-8d4f-f94374b9c9ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mYhHsa-2GZqR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qMFs3YZvtR71",
        "outputId": "399f2364-0701-488f-deae-897f34e6a594"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Would you rather get a gift that you knew what...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is the internet ruining people's ability to co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Permanganate?\\nSuppose permanganate was used t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If Rock-n-Roll is really the work of the devil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Has anyone purchased software to watch TV on y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  Would you rather get a gift that you knew what...\n",
              "1  Is the internet ruining people's ability to co...\n",
              "2  Permanganate?\\nSuppose permanganate was used t...\n",
              "3  If Rock-n-Roll is really the work of the devil...\n",
              "4  Has anyone purchased software to watch TV on y..."
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/hw6_train.csv\")\n",
        "train_text=train[\"text\"]\n",
        "\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/hw6_test.csv\")\n",
        "test_label = test[\"label\"]\n",
        "test_text = test[\"text\"]\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnrYHVDqtR7z",
        "outputId": "4778067b-321e-47e4-9568-736dc5e74848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4000, 6861)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
        "                             min_df=5) \n",
        "\n",
        "dtm= tfidf_vect.fit_transform(train[\"text\"])\n",
        "print (dtm.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.cluster import KMeansClusterer, \\\n",
        "cosine_distance\n",
        "\n",
        "# set number of clusters\n",
        "num_clusters=3\n",
        "\n",
        "clusterer = KMeansClusterer(num_clusters, \\\n",
        "                            cosine_distance, \\\n",
        "                            repeats=20)\n",
        "\n",
        "clusters = clusterer.cluster(dtm.toarray(), \\\n",
        "                             assign_clusters=True)"
      ],
      "metadata": {
        "id": "jOxnFrUCGkrr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BMsuxrtR73"
      },
      "source": [
        "## Q2: Clustering by Gaussian Mixture Model\n",
        "\n",
        "In this task, you'll re-do the clustering using a Gaussian Mixture Model. Call this function  `cluster_gmm(train_text, test_text, text_label)`. \n",
        "\n",
        "Write your analysis on the following:\n",
        "- How did you pick the parameters such as the number of clusters, variance type etc.?\n",
        "- Compare to Kmeans in Q1, do you achieve better preformance by GMM? \n",
        "\n",
        "- Note, like KMean, be sure to use different initial means (i.e. `n_init` parameter) when fitting the model to achieve the model stability "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODQnzomztR74"
      },
      "outputs": [],
      "source": [
        "# Map cluster id to true labels by \"majority vote\"\n",
        "cluster_dict={0:1,\\\n",
        "              1:2,\\\n",
        "              2:3,3:7}\n",
        "\n",
        "predicted_target=[cluster_dict[i] \\\n",
        "                  for i in predicted]\n",
        "\n",
        "print(metrics.classification_report\\\n",
        "      (test[\"label\"], predicted_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5vF1egmtR76"
      },
      "source": [
        "## Q3: Clustering by LDA \n",
        "\n",
        "In this task, you'll re-do the clustering using LDA. Call this function `cluster_lda(train_text, test_text, text_label)`. \n",
        "\n",
        "However, since LDA returns topic mixture for each document, you `assign the topic with highest probability to each test document`, and then measure the performance as in Q1\n",
        "\n",
        "In addition, within the function, please print out the top 30 words for each topic\n",
        "\n",
        "Finally, please analyze the following:\n",
        "- Based on the top words of each topic, could you assign a meaningful name to each topic?\n",
        "- Although the test subset shows there are 4 clusters, without this information, how do you choose the number of topics? \n",
        "- Does your LDA model achieve better performance than KMeans or GMM?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S4jdP7btR76",
        "outputId": "00e0607d-89a4-4b3c-c998-362f38255a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "print(stop)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages for LSI\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gscHNZ1pHcuB",
        "outputId": "8bba6b41-43bd-4259-dfb3-f685f74b83f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install mglearn\n",
        "import mglearn\n",
        "import os\n",
        "import glob\n",
        "import pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfLJ3L3pHwKE",
        "outputId": "7e10eed4-ffbb-4896-924a-312f3c26ab19"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mglearn\n",
            "  Downloading mglearn-0.1.9.tar.gz (540 kB)\n",
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 51 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 102 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 112 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 225 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 245 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 358 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 378 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 430 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 450 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 471 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 491 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 512 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 540 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mglearn) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.1.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from mglearn) (7.1.2)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.7/dist-packages (from mglearn) (0.11.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from mglearn) (2.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mglearn) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mglearn) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mglearn) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mglearn) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->mglearn) (2018.9)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->mglearn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->mglearn) (3.0.0)\n",
            "Building wheels for collected packages: mglearn\n",
            "  Building wheel for mglearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582637 sha256=85c7ab0ba88171f1cd34361958fe01b93e2ca6bb1728ac5cc8e5215ad87564f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/17/e1/1720d6dcd70187b6b6c3750cb3508798f2b1d57c9d3214b08b\n",
            "Successfully built mglearn\n",
            "Installing collected packages: mglearn\n",
            "Successfully installed mglearn-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_tokens = [w for w in word_tokens if not w in stop_words if len(w) > 2]\n",
        "    return filtered_tokens\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize, use_idf=True,smooth_idf=True)\n",
        "\n",
        "svd_model = TruncatedSVD(n_components=10, algorithm='randomized',n_iter=10)\n",
        "\n",
        "svd_transformer = Pipeline([('tfidf', vectorizer), ('svd', svd_model)])\n",
        "\n",
        "svd_matrix = svd_transformer.fit_transform(train.text)\n",
        "\n",
        "tfidf = svd_transformer.steps[0][-1]\n",
        "voc = tfidf.get_feature_names()\n",
        "\n",
        "features_names = np.array(voc)\n",
        "\n",
        "sorting = np.argsort(svd_model.components_, axis=1)[:, ::-1]\n",
        "\n",
        "mglearn.tools.print_topics(topics=range(10), feature_names=features_names,\n",
        "                           sorting=sorting, topics_per_chunk=5, n_words=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS4ibe6KHMnv",
        "outputId": "d00c5eb2-e233-4286-b859-0edd8d8832ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
            "--------      --------      --------      --------      --------      \n",
            "...           ...           ...           weight        credit        \n",
            "n't           weight        god           god           god           \n",
            "get           eat           jesus         eat           weight        \n",
            "people        diet          bible         lose          business      \n",
            "would         fat           believe       diet          money         \n",
            "like          lose          religion      fat           pay           \n",
            "know          eating        christians    exercise      card          \n",
            "one           water         people        body          job           \n",
            "god           exercise      man           eating        lose          \n",
            "think         body          christian     water         loan          \n",
            "want          calories      spirit        calories      eat           \n",
            "good          pounds        church        jesus         want          \n",
            "help          healthy       love          day           diet          \n",
            "time          foods         christ        food          good          \n",
            "really        drink         faith         pounds        fat           \n",
            "need          food          holy          muscle        company       \n",
            "could         loose         word          healthy       need          \n",
            "make          sugar         world         bible         work          \n",
            "way           avoid         sin           drink         jesus         \n",
            "life          pills         religions     foods         paid          \n",
            "work          rice          heaven        loose         score         \n",
            "also          muscle        life          sugar         debt          \n",
            "see           help          allah         meals         home          \n",
            "take          .but          true          energy        tax           \n",
            "feel          lost          christianity  believe       calories      \n",
            "much          week          worship       religion      bank          \n",
            "something     protein       say           burn          free          \n",
            "find          per           hell          blood         bible         \n",
            "say           ..i           question      meal          exercise      \n",
            "even          meals         islam         gain          eating        \n",
            "may           dont          think         losing        account       \n",
            "things        fast          lord          loss          get           \n",
            "weight        intake        truth         week          plan          \n",
            "day           watch         gay           protein       buy           \n",
            "right         thats         created       carbs         mortgage      \n",
            "many          normal        son           pills         cards         \n",
            "going         sauces        muslims       christians    report        \n",
            "try           start         name          cardio        pounds        \n",
            "years         milk          jews          per           income        \n",
            "well          pasta         earth         metabolism    bankruptcy    \n",
            "question      everyday      evil          stomach       sell          \n",
            "someone       carbs         book          times         help          \n",
            "tell          lol           others        lbs           companies     \n",
            "believe       veggies       says          exercising    start         \n",
            "back          lots          human         low           program       \n",
            "thing         low           father        reduce        market        \n",
            "person        day           said          fruits        find          \n",
            "best          hungry        pray          weights       food          \n",
            "job           amount        mean          intake        interest      \n",
            "'ve           wheat         catholic      spirit        payment       \n",
            "\n",
            "\n",
            "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
            "--------      --------      --------      --------      --------      \n",
            "water         god           credit        question      people        \n",
            "energy        credit        card          http          blood         \n",
            "light         pain          people        answer        credit        \n",
            "credit        doctor        loan          n't           religion      \n",
            "earth         blood         energy        answers       many          \n",
            "god           get           score         credit        type          \n",
            "mass          card          n't           re-post       also          \n",
            "business      help          black         help          cancer        \n",
            "gas           back          debt          answered      doctor        \n",
            "air           n't           pay           questions     may           \n",
            "speed         could         think         received      body          \n",
            "sun           period        like          please        person        \n",
            "heat          normal        light         yahoo         bible         \n",
            "pressure      loan          gay           consider      disease       \n",
            "would         test          water         know          one           \n",
            "card          days          cards         could         insulin       \n",
            "one           cause         mass          points        card          \n",
            "used          take          feel          period        gay           \n",
            "...           taking        rate          'answer       test          \n",
            "temperature   jesus         weight        ones.\\n2      sugar         \n",
            "force         skin          mortgage      //www.answers.com/\\nbartlebyreligions     \n",
            "number        months        bankruptcy    'best         cells         \n",
            "universe      surgery       life          //en.wikipedia.org/wiki/main_page\\n\\nsincepain          \n",
            "gravity       teeth         report        you.\\n\\nanswers.comdifferent     \n",
            "company       heart         payment       //www.bartleby.com/\\nyahoochristians    \n",
            "use           score         say           re-posting    cause         \n",
            "space         infection     earth         //www.howstuffworks.com/\\nwikipediaheart         \n",
            "name          pay           account       //education.yahoo.com/reference/\\nhowstuffworksmedical       \n",
            "star          son           sun           grammatical   religious     \n",
            "rate          debt          matter        worded        risk          \n",
            "state         cancer        bank          category      name          \n",
            "oxygen        symptoms      speed         expired       christian     \n",
            "http          sleep         interest      errors        language      \n",
            "two           weeks         gravity       spelling      rate          \n",
            "information   ago           universe      meets         islam         \n",
            "surface       anything      paid          regards       health        \n",
            "amount        pregnancy     want          listing       called        \n",
            "system        pressure      love          newer         jesus         \n",
            "constant      control       apr           weight        group         \n",
            "form          mouth         person        reference     common        \n",
            "small         pills         bad           appears       used          \n",
            "charge        side          know          sites         loan          \n",
            "loan          year          women         card          normal        \n",
            "equation      bad           car           favorite      score         \n",
            "using         told          never         give          symptoms      \n",
            "money         problems      way           ask           read          \n",
            "density       went          lose          \\n\\nif        others        \n",
            "also          tooth         force         \\n\\n1         christianity  \n",
            "salt          may           hole          following     information   \n",
            "pay           nerve         lenders       seem          diet          \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANSWERS\n",
        "\n",
        "Q. Based on the top words of each topic, could you assign a meaningful name to each topic?\n",
        "ans : yes we can do that and is achievable\n",
        "\n",
        "Q. Although the test subset shows there are 4 clusters, without this information, how do you choose the number of topics?\n",
        "ans: \n",
        "\n",
        "1. Elbow method \n",
        "\n",
        "Compute clustering algorithm (e.g., k-means clustering) for different values of k. \n",
        "\n",
        "For instance, by varying k from 1 to 10 clusters.\n",
        "\n",
        "For each k, calculate the total within-cluster sum of square (wss).\n",
        "\n",
        "Plot the curve of wss according to the number of clusters k.\n",
        "\n",
        "The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.\n",
        "\n",
        "2. Average silhouette method\n",
        "\n",
        "Compute clustering algorithm (e.g., k-means clustering) for different values of k. \n",
        "For instance, by varying k from 1 to 10 clusters.\n",
        "\n",
        "For each k, calculate the average silhouette of observations (avg.sil).\n",
        "\n",
        "Plot the curve of avg.sil according to the number of clusters k.\n",
        "\n",
        "The location of the maximum is considered as the appropriate number of clusters.\n",
        "\n",
        "3. Gap statistic method\n",
        "\n",
        "Cluster the observed data, varying the number of clusters from k = 1, …, kmax, and compute the corresponding total within intra-cluster variation Wk.\n",
        "\n",
        "Generate B reference data sets with a random uniform distribution. Cluster each of these reference data sets with varying number of clusters k = 1, …, kmax, and compute the corresponding total within intra-cluster variation Wkb.\n",
        "\n",
        "Compute the estimated gap statistic as the deviation of the observed Wk value from its expected value Wkb under the null hypothesis: Gap(k)=1B∑b=1Blog(W∗kb)−log(Wk).\n",
        "\n",
        "Compute also the standard deviation of the statistics.\n",
        "\n",
        "Choose the number of clusters as the smallest value of k such that the gap statistic is within one standard deviation of the gap at k+1: Gap(k)≥Gap(k + 1)−sk + 1.\n",
        "\n",
        "Q. Does your LDA model achieve better performance than KMeans or GMM\n",
        "ans: yes "
      ],
      "metadata": {
        "id": "3nDzB3RgIBuy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoazvAuBtR77"
      },
      "source": [
        "## Q4 (Bonus): Topic Coherence and Separation\n",
        "\n",
        "For the LDA model you obtained at Q3, can you measure the coherence and separation of topics? Try different model parameters (e.g. number of topics, $\\alpha$) to see which one gives you the best separation and coherence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1J9XgdstR77"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Homework_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}