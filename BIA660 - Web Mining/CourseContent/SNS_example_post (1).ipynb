{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>An example of SNS</center>\n",
    "\n",
    "- Threadless.com is a crowdsouring website for graphic designs.\n",
    "- Desginers submit artworks and recieve ratings from the community within a seven-day period. \n",
    "- Designs with the best scores will be selected to print on T-shirts and other products for sale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping objectives\n",
    "\n",
    "- Get a sample of users and artifacts. \n",
    "- Scrape artifact-level features.\n",
    "- Scrape user-level features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.threadless.com/designs/archive?page=1', 'https://www.threadless.com/designs/archive?page=2', 'https://www.threadless.com/designs/archive?page=3', 'https://www.threadless.com/designs/archive?page=4', 'https://www.threadless.com/designs/archive?page=5', 'https://www.threadless.com/designs/archive?page=6', 'https://www.threadless.com/designs/archive?page=7', 'https://www.threadless.com/designs/archive?page=8', 'https://www.threadless.com/designs/archive?page=9', 'https://www.threadless.com/designs/archive?page=10']\n"
     ]
    }
   ],
   "source": [
    "# Get ten urls of pages as a sample of latest artifacts.\n",
    "\n",
    "link=\"https://www.threadless.com/designs/archive?page=\"\n",
    "num=list(range(1,11))\n",
    "pages=[]\n",
    "for i in num:\n",
    "    page=link+str(i)\n",
    "    pages.append(page)\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on page https://www.threadless.com/designs/archive?page=1\n"
     ]
    }
   ],
   "source": [
    "# Get urls of all the designs in these ten pages\n",
    "# To reduce the load to their server, will demonnstrate one page\n",
    "\n",
    "designs=[]\n",
    "for i in pages[:1]:\n",
    "    print('working on page'+str(' ')+str(i))\n",
    "    response=requests.get(i)\n",
    "    soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "    links=soup.find('ol',class_='feed-archive th-grided')\n",
    "    li=links.find_all('li',class_=\"old\")\n",
    "    for j in li:\n",
    "        name=j.find(\"a\")[\"href\"]\n",
    "        #print(name)\n",
    "        designs.append(name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/designs/handle-with-care-13',\n",
       " '/designs/scary-reaper',\n",
       " '/designs/pride-of-culture',\n",
       " '/designs/cyber-tiger-3',\n",
       " '/designs/washer-sprites']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "designs[:5]\n",
    "\n",
    "# can write out the sample of artifacts \n",
    "# with open('designs.csv', 'w') as csvfile:\n",
    "#    writer=csv.writer(csvfile, delimiter=',')\n",
    "#    writer.writerows(zip(designs))\n",
    "\n",
    "\n",
    "# read in your sample\n",
    "# raw_data_file = open(\"designs.csv\", 'r')\n",
    "# csv_data_file = csv.reader(raw_data_file, delimiter=',')\n",
    "# designs = []\n",
    "# for line in csv_data_file:\n",
    "#     print(line[0])\n",
    "#     designs.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Handle with care', 'Scary Reaper', 'PRIDE OF CULTURE', 'cyber tiger', 'Washer Sprites']\n",
      "['Tink_M', 'Base26', 'tuytuyartwork', 'tuytuyartwork', 'Getsousa']\n",
      "['2.90', '2.17', '2.67', '1.83', '3.89']\n",
      "['10', '6', '9', '6', '9']\n"
     ]
    }
   ],
   "source": [
    "# Get artifact level features\n",
    "# For each design, get title, author, average score, number of scores, challenge name\n",
    "\n",
    "titles=[]\n",
    "authors=[]\n",
    "scores=[]\n",
    "total_scores=[]\n",
    "\n",
    "for i in designs:\n",
    "    try:\n",
    "        url=\"https://www.threadless.com\"+i\n",
    "        response=requests.get(url)\n",
    "        soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        ##title\n",
    "        title=soup.find('div',class_='submission-title')\n",
    "        title=title.find(\"h1\").text\n",
    "        titles.append(title)\n",
    "        \n",
    "        ##author\n",
    "        author=soup.find('div',class_='author-block')\n",
    "        author=author.find(\"a\")[\"href\"].lstrip(\"/@\")\n",
    "        authors.append(author)\n",
    "\n",
    "        ##score\n",
    "        score=soup.find('li',class_='avg-score')\n",
    "        score=(score.find('strong')).text\n",
    "        scores.append(score)\n",
    "\n",
    "        ##total scores\n",
    "        total_score=soup.find('li',class_='total-scores')\n",
    "        total_score=(total_score.find('strong')).text\n",
    "        total_scores.append(total_score)\n",
    "    \n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(titles[0:5])\n",
    "print(authors[0:5])\n",
    "print(scores[0:5])\n",
    "print(total_scores[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threadless\n",
      " 109367 designs\n",
      "\n",
      "Horror\n",
      " 929 designs\n",
      "\n",
      "Pride Forever\n",
      " 376 designs\n",
      "\n",
      "Pride Forever\n",
      " 376 designs\n",
      "\n",
      "Threadless\n",
      " 109367 designs\n"
     ]
    }
   ],
   "source": [
    "# Question: How to scrape the challenge information?\n",
    "\n",
    "# challenge name\n",
    "# how many designs per challenge\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(authors)\n",
    "authors_unique=list(set(authors))\n",
    "print(authors_unique)\n",
    "len(authors_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['133 threads started', '1,141 designs submitted', '9,626 designs scored', 'Avg Score Given: 4.52', 'Member since 2010', 'Agimat ni Ingkong']\n",
      "[None, '17 designs submitted', '7 designs scored', 'Avg Score Given: 4.71', 'Member since 2021', 'tuytuyartwork']\n",
      "[None, '7 designs submitted', '4 designs scored', 'Avg Score Given: 4.75', 'Member since 2021', 'phalano']\n",
      "['2 threads started', '27 designs submitted', '3,727 designs scored', 'Avg Score Given: 2.49', 'Member since 2012', 'FlamingDerps']\n",
      "[None, '41 designs submitted', '234 designs scored', 'Avg Score Given: 3.82', 'Member since 2015', 'Tink_M']\n"
     ]
    }
   ],
   "source": [
    "# For the designers we found, get the summary of their experience\n",
    "full=[]\n",
    "\n",
    "for i in authors_unique[:5]:\n",
    "    url=\"https://www.threadless.com/@\"+i\n",
    "    time.sleep(5)\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content, 'lxml')\n",
    "    \n",
    "    # find all stats\n",
    "    stats=soup.find('div',class_='stats')\n",
    "    li=stats.find_all('li')\n",
    "    \n",
    "    line=[None] * 5\n",
    "    for j in li:\n",
    "        char=(j.text).strip()\n",
    "        \n",
    "        # threads\n",
    "        if re.search(\"started\",char):\n",
    "            line[0]=char\n",
    "            #line[1]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "            \n",
    "        # submitted\n",
    "        if re.search(\"submitted\",char):\n",
    "            line[1]=char   \n",
    "            #line[1]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "\n",
    "        # scored\n",
    "        if re.search(\"scored\",char):\n",
    "            line[2]=char\n",
    "            #line[2]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "        \n",
    "        # given\n",
    "        if re.search(\"Given\",char):\n",
    "            line[3]=char\n",
    "            #line[3]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "\n",
    "        # since\n",
    "        if re.search(\"since\",char):\n",
    "            line[4]=char\n",
    "            #line[4]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "    \n",
    "    line.append(i)\n",
    "    print(line)\n",
    "    full.append(line)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agimat ni Ingkong 3645 14900\n",
      "tuytuyartwork 125 25\n",
      "phalano 1 2\n",
      "FlamingDerps 83 123\n",
      "Tink_M 95 350\n"
     ]
    }
   ],
   "source": [
    "# Question: how to scrape each designers' numbers of followers and following?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the follower-followee network for each designer.\n",
    "# Can we do this with beautifulsoup? \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tink_M', 'ppmid']\n",
      "['Tink_M', 'Agimat ni Ingkong']\n"
     ]
    }
   ],
   "source": [
    "relations=[]\n",
    "\n",
    "for i in authors_unique[3:5]:\n",
    "    \n",
    "    i=i.replace(\" \",\"%20\")\n",
    "    \n",
    "    follower_url=\"https://www.threadless.com/@\"+i+\"/followers\"\n",
    "    following_url=\"https://www.threadless.com/@\"+i+\"/following\"\n",
    "\n",
    "    # close a pop ad\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"user-agent=gene\")\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    #driver.set_page_load_timeout(30)\n",
    "\n",
    "    # one's follower   \n",
    "    driver.get(follower_url)  \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # you can scroll many times if not reaching the end\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  \n",
    "    time.sleep(10)        \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html.encode('utf-8'),\"html.parser\")\n",
    "    comp=soup.find('ol',class_=\"following-list\")\n",
    "    comp=comp.find_all(\"li\")\n",
    "\n",
    "    line=[]\n",
    "    for k in comp:\n",
    "        name=k.find(\"a\")[\"href\"]\n",
    "        name=name.lstrip(\"/@\")\n",
    "        if name in authors_unique:\n",
    "            # one's followers send the following tie\n",
    "            line=[name, i]\n",
    "            print(line)\n",
    "            relations.append(line)\n",
    "    \n",
    "    # one's follwing\n",
    "    driver.get(following_url)\n",
    "    time.sleep(10)   \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")                \n",
    "    time.sleep(25)  \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html.encode('utf-8'),\"html.parser\")\n",
    "    comp=soup.find('ol',class_=\"following-list\")\n",
    "    comp=comp.find_all(\"li\")\n",
    "\n",
    "    line=[]\n",
    "    for k in comp:\n",
    "        name=k.find(\"a\")[\"href\"]\n",
    "        name=name.lstrip(\"/@\")\n",
    "        if name in authors_unique:\n",
    "            # one sends the following tie to those to follow\n",
    "            line=[i, name]\n",
    "            print(line)\n",
    "            relations.append(line)\n",
    "    driver.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
